{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqhMgAGwmLXc"
   },
   "source": [
    "# LazyGraphRAG in LangChain\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In [LazyGraphRAG](https://www.microsoft.com/en-us/research/blog/lazygraphrag-setting-a-new-standard-for-quality-and-cost/), Microsoft demonstrates significant cost and performance benefits to delaying the construction of a knowledge graph.\n",
    "This is largely because not all documents need to be analyzed.\n",
    "However, it is also benefical that documents by the time documents are analyzed the question is already known, allowing irrelevant information to be ignored. \n",
    "\n",
    "We've noticed similar cost benefits to building a document graph linking content based on simple properties such as extracted keywords compared to building a complete knowledge graph.\n",
    "For the Wikipedia dataset used in this notebook, we estimated it would have taken $70k to build a knowledege graph using the [example from LangChain](https://python.langchain.com/docs/how_to/graph_constructing/#llm-graph-transformer), while the document graph was basically free.\n",
    "\n",
    "In this notebook we demonstrate how to populate a document graph with Wikipedia articles linked based on mentions in the articles and extracted keywords.\n",
    "Keyword extraction uses a local [KeyBERT](https://maartengr.github.io/KeyBERT/) model, making it fast and cost-effective to construct these graphs.\n",
    "We'll then show how to build out a chain which does the steps of Lazy GraphRAG -- retrieving articles, extracting claims from each community, ranking and selecting the top claims, and generating an answer based on those claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6dFtwI_xmFW"
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "The following block will configure the environment from the Colab Secrets.\n",
    "To run it, you should have the following Colab Secrets defined and accessible to this notebook:\n",
    "\n",
    "- `OPENAI_API_KEY`: The OpenAI key.\n",
    "- `ASTRA_DB_API_ENDPOINT`: The Astra DB API endpoint.\n",
    "- `ASTRA_DB_APPLICATION_TOKEN`: The Astra DB Application token.\n",
    "- `LANGCHAIN_API_KEY`: Optional. If defined, will enable LangSmith tracing.\n",
    "- `ASTRA_DB_KEYSPACE`: Optional. If defined, will specify the Astra DB keyspace. If not defined, will use the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "r0-5VJWGsBM3",
    "tags": [
     "hide_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~angchain-community (/Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain-core in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (0.3.59)\n",
      "Collecting langchain-astradb\n",
      "  Downloading langchain_astradb-0.6.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: langchain-openai in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (0.3.16)\n",
      "Collecting langchain-graph-retriever\n",
      "  Downloading langchain_graph_retriever-0.8.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.5-cp311-cp311-macosx_10_9_x86_64.whl.metadata (27 kB)\n",
      "Collecting graph-rag-example-helpers\n",
      "  Downloading graph_rag_example_helpers-0.8.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langchain-core) (0.3.42)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langchain-core) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langchain-core) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langchain-core) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langchain-core) (2.11.4)\n",
      "Collecting astrapy<3.0.0,>=2.0.1 (from langchain-astradb)\n",
      "  Downloading astrapy-2.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting langchain-community>=0.3.1 (from langchain-astradb)\n",
      "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting numpy<2.0.0,>=1.26.0 (from langchain-astradb)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langchain-openai) (1.77.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: backoff>=2.2.1 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langchain-graph-retriever) (2.2.1)\n",
      "Collecting graph-retriever (from langchain-graph-retriever)\n",
      "  Downloading graph_retriever-0.8.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting immutabledict>=4.2.1 (from langchain-graph-retriever)\n",
      "  Downloading immutabledict-4.2.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: networkx>=3.4.2 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langchain-graph-retriever) (3.4.2)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp311-cp311-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp311-cp311-macosx_10_9_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-macosx_10_9_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp311-cp311-macosx_10_9_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: jinja2 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from spacy) (75.3.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting griffe>=1.5.7 (from graph-rag-example-helpers)\n",
      "  Downloading griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting httpx>=0.28.1 (from graph-rag-example-helpers)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting python-dotenv>=1.0.1 (from graph-rag-example-helpers)\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting simsimd>=6.2.1 (from graph-rag-example-helpers)\n",
      "  Downloading simsimd-6.2.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (66 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0 (from spacy)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: deprecation<2.2.0,>=2.1.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from astrapy<3.0.0,>=2.0.1->langchain-astradb) (2.1.0)\n",
      "Collecting pymongo>=3 (from astrapy<3.0.0,>=2.0.1->langchain-astradb)\n",
      "  Downloading pymongo-4.12.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: toml<0.11.0,>=0.10.2 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from astrapy<3.0.0,>=2.0.1->langchain-astradb) (0.10.2)\n",
      "Requirement already satisfied: uuid6>=2024.1.12 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from astrapy<3.0.0,>=2.0.1->langchain-astradb) (2024.7.10)\n",
      "Requirement already satisfied: colorama>=0.4 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from griffe>=1.5.7->graph-rag-example-helpers) (0.4.6)\n",
      "Requirement already satisfied: anyio in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpx>=0.28.1->graph-rag-example-helpers) (4.8.0)\n",
      "Requirement already satisfied: certifi in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpx>=0.28.1->graph-rag-example-helpers) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpx>=0.28.1->graph-rag-example-helpers) (1.0.2)\n",
      "Requirement already satisfied: idna in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpx>=0.28.1->graph-rag-example-helpers) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.28.1->graph-rag-example-helpers) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langchain-community>=0.3.1->langchain-astradb) (0.3.25)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langchain-community>=0.3.1->langchain-astradb) (2.0.38)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langchain-community>=0.3.1->langchain-astradb) (3.11.13)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langchain-community>=0.3.1->langchain-astradb) (0.5.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langchain-community>=0.3.1->langchain-astradb) (2.8.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langchain-community>=0.3.1->langchain-astradb) (0.4.0)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (0.23.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp311-cp311-macosx_10_9_x86_64.whl.metadata (15 kB)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: pytest>=8.3.4 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from graph-retriever->langchain-graph-retriever) (8.3.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.1->langchain-astradb) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.1->langchain-astradb) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.1->langchain-astradb) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.1->langchain-astradb) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.1->langchain-astradb) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.1->langchain-astradb) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.1->langchain-astradb) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community>=0.3.1->langchain-astradb) (3.26.1)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community>=0.3.1->langchain-astradb) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community>=0.3.1->langchain-astradb) (0.9.0)\n",
      "Collecting h2<5,>=3 (from httpx[http2]<1,>=0.25.2->astrapy<3.0.0,>=2.0.1->langchain-astradb)\n",
      "  Using cached h2-4.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from langchain<1.0.0,>=0.3.25->langchain-community>=0.3.1->langchain-astradb) (0.3.8)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo>=3->astrapy<3.0.0,>=2.0.1->langchain-astradb)\n",
      "  Using cached dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: iniconfig in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pytest>=8.3.4->graph-retriever->langchain-graph-retriever) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pytest>=8.3.4->graph-retriever->langchain-graph-retriever) (1.5.0)\n",
      "Requirement already satisfied: wrapt in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain-community>=0.3.1->langchain-astradb) (3.1.1)\n",
      "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy<3.0.0,>=2.0.1->langchain-astradb)\n",
      "  Using cached hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy<3.0.0,>=2.0.1->langchain-astradb)\n",
      "  Using cached hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community>=0.3.1->langchain-astradb) (1.0.0)\n",
      "Downloading langchain_astradb-0.6.0-py3-none-any.whl (69 kB)\n",
      "Downloading langchain_graph_retriever-0.8.0-py3-none-any.whl (33 kB)\n",
      "Downloading spacy-3.8.5-cp311-cp311-macosx_10_9_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading graph_rag_example_helpers-0.8.0-py3-none-any.whl (19 kB)\n",
      "Downloading astrapy-2.0.1-py3-none-any.whl (300 kB)\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp311-cp311-macosx_10_9_x86_64.whl (42 kB)\n",
      "Downloading griffe-1.7.3-py3-none-any.whl (129 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
      "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp311-cp311-macosx_10_9_x86_64.whl (26 kB)\n",
      "Downloading numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading preshed-3.0.9-cp311-cp311-macosx_10_9_x86_64.whl (132 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading simsimd-6.2.1-cp311-cp311-macosx_10_9_x86_64.whl (102 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp311-cp311-macosx_10_9_x86_64.whl (635 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m635.9/635.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.4-cp311-cp311-macosx_10_9_x86_64.whl (839 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m839.3/839.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading graph_retriever-0.8.0-py3-none-any.whl (37 kB)\n",
      "Downloading blis-1.2.1-cp311-cp311-macosx_10_9_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pymongo-4.12.1-cp311-cp311-macosx_10_9_x86_64.whl (855 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m855.9/855.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Using cached dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "Using cached h2-4.2.0-py3-none-any.whl (60 kB)\n",
      "Downloading marisa_trie-1.2.1-cp311-cp311-macosx_10_9_x86_64.whl (192 kB)\n",
      "Using cached hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Using cached hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~angchain-community (/Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: simsimd, cymem, wasabi, tqdm, spacy-loggers, spacy-legacy, smart-open, python-dotenv, numpy, murmurhash, marisa-trie, immutabledict, hyperframe, hpack, griffe, dnspython, cloudpathlib, catalogue, srsly, pymongo, preshed, language-data, httpx, h2, graph-retriever, blis, langcodes, confection, weasel, thinc, astrapy, spacy, langchain-graph-retriever, graph-rag-example-helpers, langchain-community, langchain-astradb\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.4\n",
      "    Uninstalling tqdm-4.66.4:\n",
      "      Successfully uninstalled tqdm-4.66.4\n",
      "  Attempting uninstall: python-dotenv\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~angchain-community (/Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: python-dotenv 1.0.0\n",
      "    Uninstalling python-dotenv-1.0.0:\n",
      "      Successfully uninstalled python-dotenv-1.0.0\n",
      "  Attempting uninstall: numpy\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~angchain-community (/Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.27.2\n",
      "    Uninstalling httpx-0.27.2:\n",
      "      Successfully uninstalled httpx-0.27.2\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.0.20\n",
      "    Uninstalling langchain-community-0.0.20:\n",
      "      Successfully uninstalled langchain-community-0.0.20\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~angchain-community (/Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gpt-engineer 0.3.1 requires datasets<3.0.0,>=2.17.1, but you have datasets 3.2.0 which is incompatible.\n",
      "gpt-engineer 0.3.1 requires langchain-anthropic<0.2.0,>=0.1.1, but you have langchain-anthropic 0.3.3 which is incompatible.\n",
      "gpt-engineer 0.3.1 requires langchain-community<0.3.0,>=0.2.0, but you have langchain-community 0.3.24 which is incompatible.\n",
      "gpt-engineer 0.3.1 requires pillow<11.0.0,>=10.2.0, but you have pillow 11.1.0 which is incompatible.\n",
      "gpt-engineer 0.3.1 requires regex<2024.0.0,>=2023.12.25, but you have regex 2024.11.6 which is incompatible.\n",
      "litellm 1.60.2 requires httpx<0.28.0,>=0.23.0, but you have httpx 0.28.1 which is incompatible.\n",
      "browser-use 0.1.40 requires langchain-openai==0.3.1, but you have langchain-openai 0.3.16 which is incompatible.\n",
      "browser-use 0.1.40 requires setuptools>=75.8.0, but you have setuptools 75.3.0 which is incompatible.\n",
      "datasets 3.2.0 requires fsspec[http]<=2024.9.0,>=2023.1.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed astrapy-2.0.1 blis-1.2.1 catalogue-2.0.10 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 dnspython-2.7.0 graph-rag-example-helpers-0.8.0 graph-retriever-0.8.0 griffe-1.7.3 h2-4.2.0 hpack-4.1.0 httpx-0.28.1 hyperframe-6.1.0 immutabledict-4.2.1 langchain-astradb-0.6.0 langchain-community-0.3.24 langchain-graph-retriever-0.8.0 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 numpy-1.26.4 preshed-3.0.9 pymongo-4.12.1 python-dotenv-1.1.0 simsimd-6.2.1 smart-open-7.1.0 spacy-3.8.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 tqdm-4.67.1 wasabi-1.1.3 weasel-0.4.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~angchain-community (/Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~angchain-community (/Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install modules.\n",
    "#\n",
    "# On Apple hardware, \"spacy[apple]\" will improve performance.\n",
    "%pip install \\\n",
    "    langchain-core \\\n",
    "    langchain-astradb \\\n",
    "    langchain-openai \\\n",
    "    langchain-graph-retriever \\\n",
    "    spacy \\\n",
    "    graph-rag-example-helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last package -- `graph-rag-example-helpers` -- includes some helpers for setting up environment helpers and allowing the loading of wikipedia data to be restarted if it fails.\n",
    "\n",
    "spaCy는 자연어 처리(NLP, Natural Language Processing)를 위한 오픈소스 라이브러리입니다. 주요 기능으로는:\n",
    "토큰화(Tokenization)\n",
    "품사 태깅(Part-of-speech tagging)\n",
    "개체명 인식(Named Entity Recognition, NER)\n",
    "구문 분석(Dependency parsing)\n",
    "문장 분할(Sentence segmentation)\n",
    "등을 제공합니다.\n",
    "!python -m spacy download en_core_web_sm 명령은 spaCy의 영어 언어 모델 중 하나인 'en_core_web_sm'을 다운로드하는 명령입니다.\n",
    "en: 영어 모델\n",
    "core: 기본 기능을 포함\n",
    "web: 웹 텍스트에 최적화\n",
    "sm: small 모델 (가벼운 버전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~angchain-community (/Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~angchain-community (/Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: en-core-web-sm\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~angchain-community (/Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed en-core-web-sm-3.8.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~angchain-community (/Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~angchain-community (/Users/uengine/.pyenv/versions/3.11.7/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Downloads the model used by Spacy for extracting entities.\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "# Configure import paths.\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "# Initialize environment variables.\n",
    "from graph_rag_example_helpers.env import Environment, initialize_environment\n",
    "\n",
    "initialize_environment(Environment.ASTRAPY)\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"lazy-graph-rag\"\n",
    "\n",
    "# The full dataset is ~6m documents, and takes hours to load.\n",
    "# The short dataset is 1000 documents and loads quickly.\n",
    "# Change this to `True` to use the larger dataset.\n",
    "USE_SHORT_DATASET = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading Data\n",
    "\n",
    "# 먼저, `AstraDBVectorStore`에 위키백과 데이터를 로드하는 방법을 보여드리겠습니다. 이를 위해 언급된 기사와 키워드를 메타데이터 필드로 사용합니다.\n",
    "# 이 섹션에서는 실제로 그래프에 특별한 작업을 수행하지 않습니다. 우리는 단지 우리의 내용을 유용하게 설명하는 필드를 메타데이터에 채워넣는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Documents from Wikipedia Articles\n",
    "# 먼저, 가져올 `LangChain` `Document`s를 생성해야 합니다.\n",
    "# \n",
    "# 이를 위해, [2wikimultihop](https://github.com/Alab-NII/2wikimultihop?tab=readme-ov-file#new-update-april-7-2021)에서 다운로드한 JSON 파일의 줄을 변환하는 코드를 작성합니다. 이를 통해 `Document`를 생성합니다.\n",
    "# 이 파일에 있는 정보를 사용하여 `id`와 `metadata[\"mentions\"]`를 채워넣습니다.\n",
    "# \n",
    "# 다음으로, `SpacyNERTransformer`를 통해 문서를 실행하여 기사에서 언급된 엔티티를 `metadata[\"entities\"]`에 채워넣습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8u4lD-AqDMMs"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections.abc import Iterator\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_graph_retriever.transformers.spacy import (\n",
    "    SpacyNERTransformer,\n",
    ")\n",
    "\n",
    "\n",
    "def parse_document(line: bytes) -> Document:\n",
    "    \"\"\"Reads one JSON line from the wikimultihop dump.\"\"\"\n",
    "    para = json.loads(line)\n",
    "\n",
    "    id = para[\"id\"]\n",
    "    title = para[\"title\"]\n",
    "\n",
    "    # Use structured information (mentioned Wikipedia IDs) as metadata.\n",
    "    mentioned_ids = [id for m in para[\"mentions\"] for m in m[\"ref_ids\"] or []]\n",
    "\n",
    "    return Document(\n",
    "        id=id,\n",
    "        page_content=\" \".join(para[\"sentences\"]),\n",
    "        metadata={\n",
    "            \"mentions\": mentioned_ids,\n",
    "            \"title\": title,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "NER_TRANSFORMER = SpacyNERTransformer(\n",
    "    limit=1000,\n",
    "    exclude_labels={\"CARDINAL\", \"MONEY\", \"QUANTITY\", \"TIME\", \"PERCENT\", \"ORDINAL\"},\n",
    ")\n",
    "\n",
    "\n",
    "# Load data in batches, using GLiNER to extract entities.\n",
    "def prepare_batch(lines: Iterator[str]) -> Iterator[Document]:\n",
    "    # Parse documents from the batch of lines.\n",
    "    docs = [parse_document(line) for line in lines]\n",
    "\n",
    "    docs = NER_TRANSFORMER.transform_documents(docs)\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the AstraDBVectorStore\n",
    "Next, we create the Vector Store we're going to load these documents into.\n",
    "In our case, we use DataStax Astra DB with Open AI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_astradb import AstraDBVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "COLLECTION = \"lazy_graph_rag_short\" if USE_SHORT_DATASET else \"lazy_graph_rag\"\n",
    "store = AstraDBVectorStore(\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    collection_name=COLLECTION,\n",
    "    pre_delete_collection=USE_SHORT_DATASET,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data into the Store\n",
    "Next, we perform the actual loading.\n",
    "This takes a while, so we use a helper utility to persist which batches have been written so we can resume if there are any failures.\n",
    "\n",
    "On OS X, it is useful to run `caffeinate -dis` in a shell to prevent the machine from going to sleep and seems to reduce errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드는 Wikipedia 데이터를 로드하고 처리하는 명령입니다. 구체적으로 살펴보면:\n",
    "\n",
    "```python\n",
    "await aload_2wikimultihop(\n",
    "    limit=100 if USE_SHORT_DATASET else None,\n",
    "    full_para_with_hyperlink_zip_path=PARA_WITH_HYPERLINK_ZIP,\n",
    "    store=store,\n",
    "    batch_prepare=prepare_batch,\n",
    ")\n",
    "```\n",
    "\n",
    "1. `aload_2wikimultihop`: 2wikimultihop 데이터셋을 비동기적으로 로드하는 함수입니다. 2wikimultihop은 Wikipedia 문서들의 데이터셋으로, 문서들 간의 하이퍼링크 관계 정보를 포함하고 있습니다.\n",
    "\n",
    "2. 주요 매개변수:\n",
    "   - `limit=100 if USE_SHORT_DATASET else None`: \n",
    "     - `USE_SHORT_DATASET`가 True이면 100개의 문서만 로드\n",
    "     - False이면 전체 데이터셋(약 6백만 문서) 로드\n",
    "   \n",
    "   - `full_para_with_hyperlink_zip_path=PARA_WITH_HYPERLINK_ZIP`:\n",
    "     - Wikipedia 데이터가 포함된 ZIP 파일의 경로\n",
    "\n",
    "   - `store=store`: \n",
    "     - 처리된 데이터를 저장할 AstraDB 벡터 저장소\n",
    "\n",
    "   - `batch_prepare=prepare_batch`:\n",
    "     - 문서를 처리하는 함수로, 각 문서에서 엔티티를 추출하고 메타데이터를 준비\n",
    "\n",
    "이 함수는 다음과 같은 작업을 수행합니다:\n",
    "1. Wikipedia 문서들을 ZIP 파일에서 읽어옴\n",
    "2. 각 문서에서 관련 정보(제목, 본문, 하이퍼링크 등) 추출\n",
    "3. SpaCy를 사용하여 각 문서에서 엔티티(사람, 장소, 조직 등) 추출\n",
    "4. 처리된 데이터를 AstraDB 벡터 저장소에 저장\n",
    "\n",
    "이렇게 저장된 데이터는 나중에 LazyGraphRAG 시스템에서 문서 간의 관계를 분석하고 질문에 답변하는 데 사용됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "hide_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from ../../data/para_with_hyperlink_short.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "from graph_rag_example_helpers.datasets.wikimultihop import aload_2wikimultihop\n",
    "\n",
    "# Path to the file `para_with_hyperlink.zip`.\n",
    "# See instructions here to download from\n",
    "# [2wikimultihop](https://github.com/Alab-NII/2wikimultihop?tab=readme-ov-file#new-update-april-7-2021).\n",
    "PARA_WITH_HYPERLINK_ZIP = os.path.join(os.getcwd(), \"para_with_hyperlink.zip\")\n",
    "\n",
    "await aload_2wikimultihop(\n",
    "    limit=100 if USE_SHORT_DATASET else None,\n",
    "    full_para_with_hyperlink_zip_path=PARA_WITH_HYPERLINK_ZIP,\n",
    "    store=store,\n",
    "    batch_prepare=prepare_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we've created a `VectorStore` with the Wikipedia articles.\n",
    "Each article is associated with metadata identifying other articles it mentions and entities from the article.\n",
    "\n",
    "As is, this is useful for performing a vector search filtered to articles mentioning a specific term or performing an entity seach on the documents.\n",
    "The library `langchain-graph-retriever` makes this even more useful by allowing articles to be traversed based on relationships such as articles mentioned in the current article (or mentioning the current article) or articles providing more information on the entities mentioned in the current article.\n",
    "\n",
    "In the next section we'll see not just how we can use the relationships in the metadata to retrieve more articles, but we'll go a step further and perform Lazy GraphRAG to extract relevant claims from both the similar and related articles and use the most relevant claims to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Lazy Graph RAG via Hierarchical Summarization\n",
    "\n",
    "As we've noted before, eagerly building a knowledge graph is prohibitively expensive.\n",
    "Microsoft seems to agree, and recently introduced LazyGraphRAG, which enables GraphRAG to be performed late -- after a query is retrieved.\n",
    "\n",
    "We implement the LazyGraphRAG technique using the traversing retrievers as follows:\n",
    "\n",
    "1. Retrieve a good number of nodes using a traversing retrieval.\n",
    "2. Identify communities in the retrieved sub-graph.\n",
    "3. Extract claims from each community relevant to the query using an LLM.\n",
    "4. Rank each of the claims based on the relevance to the question and select the top claims.\n",
    "5. Generate an answer to the question based on the extracted claims.\n",
    "\n",
    "이전에 언급했듯이, 지식 그래프를 적극적으로 구축하는 것은 엄청나게 비용이 듭니다.\n",
    "마이크로소프트도 동의하며, 최근에 LazyGraphRAG를 도입했습니다. 이는 쿼리가 검색된 후에 GraphRAG를 수행할 수 있게 합니다.\n",
    "우리는 다음과 같은 방법으로 트래버싱 리트리버를 사용하여 LazyGraphRAG 기법을 구현합니다:\n",
    "\n",
    "1. 트래버싱 리트리벌을 사용하여 적절한 수의 노드를 검색합니다.\n",
    "2. 검색된 하위 그래프에서 커뮤니티를 식별합니다.\n",
    "3. LLM을 사용하여 쿼리와 관련된 각 커뮤니티에서 주장을 추출합니다.\n",
    "4. 질문에 대한 관련성을 기준으로 각 주장을 순위付け하고, 최상위 주장을 선택합니다.\n",
    "5. 추출된 주장에 기반하여 질문에 대한 답을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain for Extracting Claims\n",
    "\n",
    "The first thing we do is create a chain that produces the claims. Given an input containing the question and the retrieved communities, it applies an LLM in parallel extracting claims from each community.\n",
    "\n",
    "A claim is just a string representing the statement and the `source_id` of the document. We request structured output so we get a list of claims.\n",
    "\n",
    "첫 번째로, 주장을 생성하는 체인을 만듭니다. 질문과 검색된 커뮤니티를 포함하는 입력이 주어지면, 각 커뮤니티에서 주장을 병렬로 추출하는 LLM을 적용합니다.\n",
    " \n",
    "주장은 문서의 `source_id`와 문장으로 구성된 단순한 문자열입니다. 구조화된 출력을 요청하여 주장의 목록을 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "from operator import itemgetter\n",
    "from typing import TypedDict\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Claim(BaseModel):\n",
    "    \"\"\"Representation of an individual claim from a source document(s).\"\"\"\n",
    "\n",
    "    claim: str = Field(description=\"The claim from the original document(s).\")\n",
    "    source_id: str = Field(description=\"Document ID containing the claim.\")\n",
    "\n",
    "\n",
    "class Claims(BaseModel):\n",
    "    \"\"\"Claims extracted from a set of source document(s).\"\"\"\n",
    "\n",
    "    claims: list[Claim] = Field(description=\"The extracted claims.\")\n",
    "\n",
    "\n",
    "MODEL = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "CLAIMS_MODEL = MODEL.with_structured_output(Claims)\n",
    "\n",
    "CLAIMS_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
    "Extract claims from the following related documents.\n",
    "\n",
    "Only return claims appearing within the specified documents.\n",
    "If no documents are provided, do not make up claims or documents.\n",
    "\n",
    "Claims (and scores) should be relevant to the question.\n",
    "Don't include claims from the documents if they are not directly or indirectly\n",
    "relevant to the question.\n",
    "\n",
    "If none of the documents make any claims relevant to the question, return an\n",
    "empty list of claims.\n",
    "\n",
    "If multiple documents make similar claims, include the original text of each as\n",
    "separate claims. Score the most useful and authoritative claim higher than\n",
    "similar, lower-quality claims.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "{formatted_documents}\n",
    "\"\"\")\n",
    "\n",
    "# TODO: Few-shot examples? Possibly with a selector?\n",
    "\n",
    "\n",
    "def format_documents_with_ids(documents: Iterable[Document]) -> str:\n",
    "    formatted_docs = \"\\n\\n\".join(\n",
    "        f\"Document ID: {doc.id}\\nContent: {doc.page_content}\" for doc in documents\n",
    "    )\n",
    "    return formatted_docs\n",
    "\n",
    "\n",
    "CLAIM_CHAIN = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"formatted_documents\": itemgetter(\"documents\")\n",
    "            | RunnableLambda(format_documents_with_ids),\n",
    "        }\n",
    "    )\n",
    "    | CLAIMS_PROMPT\n",
    "    | CLAIMS_MODEL\n",
    ")\n",
    "\n",
    "\n",
    "class ClaimsChainInput(TypedDict):\n",
    "    question: str\n",
    "    communities: Iterable[Iterable[Document]]\n",
    "\n",
    "\n",
    "@chain\n",
    "async def claims_chain(input: ClaimsChainInput) -> Iterable[Claim]:\n",
    "    question = input[\"question\"]\n",
    "    communities = input[\"communities\"]\n",
    "\n",
    "    # TODO: Use openai directly so this can use the batch API for performance/cost?\n",
    "    community_claims = await CLAIM_CHAIN.abatch(\n",
    "        [{\"question\": question, \"documents\": community} for community in communities]\n",
    "    )\n",
    "    return [claim for community in community_claims for claim in community.claims]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableParallel`은 여기서 두 가지 작업을 병렬로 처리합니다:\n",
    "\n",
    "1. `\"question\": itemgetter(\"question\")`:\n",
    "   - 입력에서 질문(question)을 추출하는 작업\n",
    "\n",
    "2. `\"formatted_documents\": itemgetter(\"documents\") | RunnableLambda(format_documents_with_ids)`:\n",
    "   - 입력에서 문서(documents)를 추출하고\n",
    "   - 추출된 문서들을 포맷팅하는 작업 (`format_documents_with_ids` 함수를 통해)\n",
    "\n",
    "이 두 작업이 병렬로 실행되어 다음과 같은 구조의 딕셔너리를 생성합니다:\n",
    "```python\n",
    "{\n",
    "    \"question\": \"추출된 질문\",\n",
    "    \"formatted_documents\": \"ID와 함께 포맷팅된 문서들\"\n",
    "}\n",
    "```\n",
    "\n",
    "이렇게 생성된 딕셔너리는 파이프라인(`|`)을 통해:\n",
    "1. `CLAIMS_PROMPT`로 전달되어 프롬프트 템플릿이 채워지고\n",
    "2. 최종적으로 `CLAIMS_MODEL`에 의해 처리되어 관련 주장(claims)들이 추출됩니다.\n",
    "\n",
    "이러한 병렬 처리는 성능 최적화를 위한 것으로, 질문 추출과 문서 포맷팅이 서로 독립적인 작업이므로 동시에 처리할 수 있게 합니다.\n",
    "\n",
    "`itemgetter`는 Python의 `operator` 모듈에서 제공하는 함수로, 주어진 키나 인덱스를 사용하여 객체에서 항목을 추출하는 콜러블(callable) 객체를 생성합니다.\n",
    "\n",
    "예를 들어 설명하면:\n",
    "\n",
    "```python\n",
    "from operator import itemgetter\n",
    "\n",
    "# 딕셔너리의 경우\n",
    "data = {\"question\": \"What is Python?\", \"documents\": [\"doc1\", \"doc2\"]}\n",
    "get_question = itemgetter(\"question\")\n",
    "result = get_question(data)  # \"What is Python?\"\n",
    "\n",
    "# 리스트의 경우\n",
    "data = [\"a\", \"b\", \"c\"]\n",
    "get_first = itemgetter(0)\n",
    "result = get_first(data)  # \"a\"\n",
    "```\n",
    "\n",
    "코드에서 사용된 예시를 보면:\n",
    "```python\n",
    "\"question\": itemgetter(\"question\")\n",
    "```\n",
    "이 부분은:\n",
    "1. 입력 데이터에서 \"question\" 키를 가진 값을 추출하는 함수를 생성\n",
    "2. 예를 들어 입력이 `{\"question\": \"Why is sky blue?\", \"documents\": [...]}` 라면\n",
    "3. `\"Why is sky blue?\"` 값을 추출\n",
    "\n",
    "```python\n",
    "\"formatted_documents\": itemgetter(\"documents\")\n",
    "```\n",
    "이 부분은:\n",
    "1. 입력 데이터에서 \"documents\" 키를 가진 값을 추출하는 함수를 생성\n",
    "2. 추출된 문서들은 이후 `format_documents_with_ids` 함수를 통해 포맷팅됨\n",
    "\n",
    "`itemgetter`의 장점:\n",
    "1. 간결한 문법: `lambda x: x[\"question\"]` 대신 `itemgetter(\"question\")`처럼 간단하게 표현\n",
    "2. 성능: `lambda` 함수보다 더 효율적\n",
    "3. 여러 키/인덱스 동시 추출 가능: `itemgetter(\"a\", \"b\")(data)` 처럼 사용 가능\n",
    "\n",
    "이 코드에서는 입력 데이터에서 필요한 부분(질문과 문서)을 효율적으로 추출하기 위해 `itemgetter`를 사용하고 있습니다.\n",
    "\n",
    "네, 정확히 맞습니다. `CLAIM_CHAIN`이 실제로 실행되는 부분입니다. 코드를 자세히 분석해보면:\n",
    "\n",
    "```python\n",
    "community_claims = await CLAIM_CHAIN.abatch(\n",
    "    [{\"question\": question, \"documents\": community} for community in communities]\n",
    ")\n",
    "```\n",
    "\n",
    "1. `communities`는 문서 그룹들의 리스트입니다.\n",
    "\n",
    "2. 리스트 컴프리헨션으로 각 커뮤니티마다 다음과 같은 딕셔너리를 생성합니다:\n",
    "   ```python\n",
    "   {\n",
    "       \"question\": question,      # 동일한 질문\n",
    "       \"documents\": community     # 각각 다른 문서 그룹\n",
    "   }\n",
    "   ```\n",
    "\n",
    "3. `.abatch()`는 이러한 입력들을 병렬로 처리합니다. 각 입력에 대해:\n",
    "   - `itemgetter`로 question과 documents를 추출\n",
    "   - documents는 `format_documents_with_ids`로 포맷팅\n",
    "   - 포맷팅된 결과로 `CLAIMS_PROMPT` 생성\n",
    "   - `CLAIMS_MODEL`로 관련 주장들을 추출\n",
    "\n",
    "4. 비동기(`await`)로 처리되어 성능을 최적화합니다.\n",
    "\n",
    "예를 들어, 3개의 커뮤니티가 있다면:\n",
    "```python\n",
    "[\n",
    "    {\"question\": \"Why is sky blue?\", \"documents\": community1},\n",
    "    {\"question\": \"Why is sky blue?\", \"documents\": community2},\n",
    "    {\"question\": \"Why is sky blue?\", \"documents\": community3}\n",
    "]\n",
    "```\n",
    "이런 형태의 입력들이 병렬로 처리되어 각 커뮤니티에서 관련된 주장들을 추출하게 됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain for Ranking Claims\n",
    "\n",
    "The next chain is used for ranking the claims so we can select the most relevant to the question.\n",
    "\n",
    "This is based on ideas from [RankRAG](https://arxiv.org/abs/2407.02485).\n",
    "Specifically, the prompt is constructed so that the next token should be `True` if the content is relevant and `False` if not.\n",
    "The probability of the token is used to determine the relevance -- `True` with a higher probability is more relevant than `True` with a lesser probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "RANK_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
    "Rank the relevance of the following claim to the question.\n",
    "Output \"True\" if the claim is relevant and \"False\" if it is not.\n",
    "Only output True or False.\n",
    "\n",
    "Question: Where is Seattle?\n",
    "\n",
    "Claim: Seattle is in Washington State.\n",
    "\n",
    "Relevant: True\n",
    "\n",
    "Question: Where is LA?\n",
    "\n",
    "Claim: New York City is in New York State.\n",
    "\n",
    "Relevant: False\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Claim: {claim}\n",
    "\n",
    "Relevant:\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def compute_rank(msg):\n",
    "    logprob = msg.response_metadata[\"logprobs\"][\"content\"][0]\n",
    "    prob = math.exp(logprob[\"logprob\"])\n",
    "    token = logprob[\"token\"]\n",
    "    if token == \"True\":\n",
    "        return prob\n",
    "    elif token == \"False\":\n",
    "        return 1.0 - prob\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected logprob: {logprob}\")\n",
    "\n",
    "\n",
    "RANK_CHAIN = RANK_PROMPT | MODEL.bind(logprobs=True) | RunnableLambda(compute_rank)\n",
    "\n",
    "\n",
    "class RankChainInput(TypedDict):\n",
    "    question: str\n",
    "    claims: Iterable[Claim]\n",
    "\n",
    "\n",
    "@chain\n",
    "async def rank_chain(input: RankChainInput) -> Iterable[Claim]:\n",
    "    # TODO: Use openai directly so this can use the batch API for performance/cost?\n",
    "    claims = input[\"claims\"]\n",
    "    ranks = await RANK_CHAIN.abatch(\n",
    "        [{\"question\": input[\"question\"], \"claim\": claim} for claim in claims]\n",
    "    )\n",
    "    rank_claims = sorted(\n",
    "        zip(ranks, claims, strict=True), key=lambda rank_claim: rank_claim[0]\n",
    "    )\n",
    "\n",
    "    return [claim for _, claim in rank_claims]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드는 LazyGraphRAG 시스템에서 주장(claims)의 관련성을 평가하고 순위를 매기는 부분입니다. 자세히 설명해드리겠습니다:\n",
    "\n",
    "1. **RANK_PROMPT (순위 매기기 프롬프트)**\n",
    "```python\n",
    "RANK_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
    "...\n",
    "\"\"\")\n",
    "```\n",
    "- 이 프롬프트는 주어진 질문에 대해 각 주장의 관련성을 평가하는 템플릿입니다\n",
    "- 예시를 포함하여 LLM이 이해하기 쉽게 구성되어 있습니다:\n",
    "  - \"Seattle은 어디에 있나요?\" 라는 질문에 \"Seattle은 Washington State에 있다\"는 주장은 관련이 있으므로 \"True\"\n",
    "  - \"LA는 어디에 있나요?\" 라는 질문에 \"New York City는 New York State에 있다\"는 주장은 관련이 없으므로 \"False\"\n",
    "\n",
    "2. **compute_rank (순위 계산 함수)**\n",
    "```python\n",
    "def compute_rank(msg):\n",
    "    logprob = msg.response_metadata[\"logprobs\"][\"content\"][0]\n",
    "    prob = math.exp(logprob[\"logprob\"])\n",
    "    token = logprob[\"token\"]\n",
    "```\n",
    "- LLM의 응답에서 확률값을 계산하는 함수입니다\n",
    "- `logprob`(로그 확률)을 실제 확률값으로 변환합니다\n",
    "- \"True\" 토큰이 나오면 그 확률을 반환하고, \"False\" 토큰이 나오면 (1 - 확률)을 반환합니다\n",
    "- 이를 통해 주장의 관련성에 대한 수치적인 점수를 얻을 수 있습니다\n",
    "\n",
    "3. **RANK_CHAIN (순위 매기기 체인)**\n",
    "```python\n",
    "RANK_CHAIN = RANK_PROMPT | MODEL.bind(logprobs=True) | RunnableLambda(compute_rank)\n",
    "```\n",
    "- 프롬프트, 모델, 순위 계산을 하나의 파이프라인으로 연결합니다\n",
    "- `logprobs=True`로 설정하여 모델이 확률값을 반환하도록 합니다\n",
    "\n",
    "4. **rank_chain (순위 매기기 비동기 함수)**\n",
    "```python\n",
    "@chain\n",
    "async def rank_chain(input: RankChainInput) -> Iterable[Claim]:\n",
    "    claims = input[\"claims\"]\n",
    "    ranks = await RANK_CHAIN.abatch([...])\n",
    "    rank_claims = sorted(zip(ranks, claims, strict=True), key=lambda rank_claim: rank_claim[0])\n",
    "```\n",
    "- 여러 주장들을 배치로 처리하여 효율적으로 순위를 매깁니다\n",
    "- 각 주장에 대한 관련성 점수를 계산하고, 이를 기준으로 정렬합니다\n",
    "- 가장 관련성 높은 주장들부터 낮은 순으로 정렬된 리스트를 반환합니다\n",
    "\n",
    "이 시스템은 RankRAG라는 논문의 아이디어를 기반으로 하며, LLM의 다음 토큰 예측 확률을 활용하여 주장의 관련성을 평가합니다. 이는 단순히 \"관련있다/없다\"의 이진 분류가 아닌, 확률에 기반한 더 세밀한 순위 매기기를 가능하게 합니다.\n",
    "\n",
    "모든 LLM이 logprobs를 제공하지는 않습니다. 이에 대해 자세히 알아보겠습니다:\n",
    "\n",
    "1. **OpenAI GPT 모델들**\n",
    "- OpenAI의 API는 logprobs를 제공합니다\n",
    "- 하지만 ChatGPT API (gpt-3.5-turbo, gpt-4 등)는 기본적으로 logprobs를 제공하지 않습니다\n",
    "- 텍스트 완성 모델(text-davinci-003 등)에서는 logprobs 파라미터를 사용할 수 있습니다\n",
    "\n",
    "2. **다른 상용 LLM 서비스들**\n",
    "- Anthropic Claude: logprobs를 제공하지 않습니다\n",
    "- Google PaLM/Gemini: logprobs를 제공하지 않습니다\n",
    "- Cohere: logprobs를 제공합니다\n",
    "- AI21: logprobs를 제공합니다\n",
    "\n",
    "3. **오픈소스 모델들**\n",
    "- 대부분의 오픈소스 모델들은 로컬에서 실행할 때 logprobs 계산이 가능합니다\n",
    "- 하지만 이는 추가적인 계산 비용이 들고 성능에 영향을 미칠 수 있습니다\n",
    "\n",
    "따라서 이 코드의 ranking 방식은 모든 LLM에서 사용할 수 있는 것은 아닙니다. logprobs를 제공하지 않는 모델을 사용할 경우 대안적인 ranking 방법을 사용해야 합니다:\n",
    "\n",
    "1. **대안적인 ranking 방법들**:\n",
    "```python\n",
    "# 예시 1: 이진 분류 방식\n",
    "def alternative_rank_1(response):\n",
    "    return 1.0 if response.strip().lower() == \"true\" else 0.0\n",
    "\n",
    "# 예시 2: 점수 기반 방식\n",
    "SCORE_PROMPT = \"\"\"\n",
    "Rate the relevance of the claim to the question on a scale of 0-10:\n",
    "Question: {question}\n",
    "Claim: {claim}\n",
    "Score (0-10):\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "2. **임베딩 기반 ranking**:\n",
    "```python\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "def compute_similarity_rank(question, claim):\n",
    "    q_embedding = embeddings.embed_query(question)\n",
    "    c_embedding = embeddings.embed_query(claim)\n",
    "    return cosine_similarity(q_embedding, c_embedding)\n",
    "```\n",
    "\n",
    "3. **Cross-encoder 기반 ranking**:\n",
    "```python\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "def compute_relevance_score(question, claim):\n",
    "    score = model.predict([question, claim])\n",
    "    return score\n",
    "```\n",
    "\n",
    "이러한 대안적인 방법들은 logprobs에 의존하지 않으면서도 효과적인 ranking을 수행할 수 있습니다. 실제 구현시에는 사용하는 LLM의 특성과 제약사항을 고려하여 적절한 ranking 방법을 선택해야 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could extend this by using an MMR-like strategy for selecting claims.\n",
    "Specifically, we could combine the relevance of the claim to the question and the diversity compared to already selected claims to select the best variety of claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LazyGraphRAG in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we produce a chain that puts everything together.\n",
    "Given a `GraphRetriever` it retrieves documents, creates communities using edges amongst the retrieved documents, extracts claims from those communities, ranks and selects the best claims, and then answers the question using those claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from graph_retriever.edges import EdgeSpec, MetadataEdgeFunction\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_graph_retriever import GraphRetriever\n",
    "from langchain_graph_retriever.document_graph import create_graph, group_by_community\n",
    "\n",
    "\n",
    "@chain\n",
    "async def lazy_graph_rag(\n",
    "    question: str,\n",
    "    *,\n",
    "    retriever: GraphRetriever,\n",
    "    model: BaseLanguageModel,\n",
    "    edges: Iterable[EdgeSpec] | MetadataEdgeFunction | None = None,\n",
    "    max_tokens: int = 1000,\n",
    "    **kwargs: Any,\n",
    ") -> str:\n",
    "    \"\"\"Retrieve claims relating to the question using LazyGraphRAG.\n",
    "\n",
    "    Returns the top claims up to the given `max_tokens` as a markdown list.\n",
    "\n",
    "    \"\"\"\n",
    "    edges = edges or retriever.edges\n",
    "    if edges is None:\n",
    "        raise ValueError(\"Must specify 'edges' in invocation or retriever\")\n",
    "\n",
    "    # 1. Retrieve documents using the (traversing) retriever.\n",
    "    documents = await retriever.ainvoke(question, edges=edges, **kwargs)\n",
    "\n",
    "    # 2. Create a graph and extract communities.\n",
    "    document_graph = create_graph(documents, edges=edges)\n",
    "    communities = group_by_community(document_graph)\n",
    "\n",
    "    # 3. Extract claims from the communities.\n",
    "    claims = await claims_chain.ainvoke(\n",
    "        {\"question\": question, \"communities\": communities}\n",
    "    )\n",
    "\n",
    "    # 4. Rank the claims and select claims up to the given token limit.\n",
    "    result_claims = []\n",
    "    tokens = 0\n",
    "\n",
    "    for claim in await rank_chain.ainvoke({\"question\": question, \"claims\": claims}):\n",
    "        claim_str = f\"- {claim.claim} (Source: {claim.source_id})\"\n",
    "\n",
    "        tokens += model.get_num_tokens(claim_str)\n",
    "        if tokens > max_tokens:\n",
    "            break\n",
    "        result_claims.append(claim_str)\n",
    "\n",
    "    return \"\\n\".join(result_claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Lazy GraphRAG in LangChain\n",
    "\n",
    "Finally, we sue the Lazy GraphRAG chain we created on the store we populated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_graph_retriever import GraphRetriever\n",
    "\n",
    "RETRIEVER = GraphRetriever(\n",
    "    store=store,\n",
    "    edges=[(\"mentions\", \"$id\"), (\"entities\", \"entities\")],\n",
    "    k=100,\n",
    "    start_k=30,\n",
    "    adjacent_k=20,\n",
    "    max_depth=3,\n",
    ")\n",
    "\n",
    "ANSWER_PROMPT = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the supporting claims.\n",
    "\n",
    "Only use information from the claims. Do not guess or make up any information.\n",
    "\n",
    "Where possible, reference and quote the supporting claims.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Claims:\n",
    "{claims}\n",
    "\"\"\")\n",
    "\n",
    "LAZY_GRAPH_RAG_CHAIN = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"claims\": RunnablePassthrough()\n",
    "        | lazy_graph_rag.bind(\n",
    "            retriever=RETRIEVER,\n",
    "            model=MODEL,\n",
    "            max_tokens=1000,\n",
    "        ),\n",
    "    }\n",
    "    | ANSWER_PROMPT\n",
    "    | MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "keep_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but it seems that the claims needed to answer the question are missing. Please provide the claims so I can help you with your question.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUESTION = \"Why are Bermudan sloop ships widely prized compared to other ships?\"\n",
    "result = await LAZY_GRAPH_RAG_CHAIN.ainvoke(QUESTION)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, below are the results to the same question using a basic RAG pattern with just vector similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "keep_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided documents do not contain any information about Bermudan sloop ships or why they might be widely prized compared to other ships.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "VECTOR_ANSWER_PROMPT = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the provided documents.\n",
    "\n",
    "Only use information from the documents. Do not guess or make up any information.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "VECTOR_CHAIN = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"documents\": (store.as_retriever() | format_docs),\n",
    "    }\n",
    "    | VECTOR_ANSWER_PROMPT\n",
    "    | MODEL\n",
    ")\n",
    "\n",
    "result = VECTOR_CHAIN.invoke(QUESTION)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LazyGraphRAG chain is great when a question needs to consider a large amount of relevant information in order to produce a thorough answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This post demonstrated how easy it is to implement Lazy GraphRAG on top of a document graph.\n",
    "\n",
    "It used `langchain-graph-retriever` from the [graph-rag project](datastax.github.io/graph-rag) to implement the document graph and graph-based retrieval on top of an existing LangChain `VectorStore`.\n",
    "This means you can focus on populating and using your `VectorStore` with useful metadata and add graph-based retrieval and even Lazy GraphRAG when you need it.\n",
    "\n",
    "**Any LangChain `VectorStore` can be used with Lazy GraphRAG without needing to change or re-ingest the stored documents.**\n",
    "Knowledge Graphs and GraphRAG shouldn't be hard or scary.\n",
    "Start simple and easily overlay edges when you need them.\n",
    "\n",
    "Graph retrievers and LazyGraph RAG work well with agents.\n",
    "You can allow the agent to retrieve differently depending on the question -- doing a vector only search for simple questions, traversing to mentioned articles for a deeper question or traversing to articles that cite this to see if there is newer information available.\n",
    "We'll show how to combine these techniques with agents in a future post.\n",
    "Until then, give `langchain-graph-retriever` a try and let us know how it goes!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
